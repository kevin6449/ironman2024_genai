{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM72FK8+aI8dZIYJZFzlnOT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevin6449/ironman2024_genai/blob/main/gen_ai_day22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 瞭解及計算符記\n",
        "Gemini 和其他生成式 AI 模型會精細處理輸入和輸出內容 符記。\n",
        "\n"
      ],
      "metadata": {
        "id": "0feJbYT-l_Qw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 關於權杖\n",
        "符記可以是單一字元 (例如 z) 或完整的字詞 (例如 cat)。詳細字詞 可分為多個符記模型使用的所有符記集 將文字分割成符記的過程 符記化。\n",
        "\n",
        "Gemini 模型的符記約為 4 個字元。 100 個符記約等於 60 到 80 個英文單字。\n",
        "\n",
        "啟用帳單功能後，對 Gemini API 的呼叫費用為 有些部分取決於輸入和輸出符記的數量，因此知道 計算符記就能派上用場"
      ],
      "metadata": {
        "id": "WlC9d_dorRr4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmZLCXuJl2gT"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "#genai.configure(api_key=\"YOUR_API_KEY\")\n",
        "\n",
        "# Configure the client library by providing your API key.\n",
        "genai.configure(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 上下文視窗\n",
        "\n",
        "透過 Gemini API 提供的模型具有以標記來衡量的上下文視窗。上下文視窗定義您可以提供多少輸入以及模型可以產生多少輸出。您可以使用 API 或檢視模型文件來確定上下文視窗的大小。\n",
        "\n"
      ],
      "metadata": {
        "id": "SdjlrSJjro5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_info = genai.get_model(\"models/gemini-1.5-flash\")\n",
        "\n",
        "# Returns the \"context window\" for the model,\n",
        "# which is the combined input and output token limits.\n",
        "print(f\"{model_info.input_token_limit=}\")\n",
        "print(f\"{model_info.output_token_limit=}\")\n",
        "# ( input_token_limit=30720, output_token_limit=2048 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "a_S98KvxmJwk",
        "outputId": "8e291711-6ba0-422e-cba2-5624a54b15e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_info.input_token_limit=1000000\n",
            "model_info.output_token_limit=8192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 計算符記數量\n",
        "所有從 Gemini API 傳入和輸出的內容都會權杖化，包括文字、圖片 檔案以及其他非文字形式的內容\n",
        "\n",
        "您可以透過下列方式計算符記：\n",
        "\n",
        " - 呼叫 count_tokens，並提供 。\n",
        "這會傳回含有 可以先進行此呼叫，再將輸入內容傳送至模型 檢查要求大小\n",
        "\n",
        " - 呼叫後，在 response 物件上使用 usage_metadata 屬性 generate_content。\n",
        "這會傳回輸入內容和輸出內容中的分詞總數： total_token_count。\n",
        "也會分別傳回輸入和輸出內容的符記數量： prompt_token_count (輸入符記) 和 candidates_token_count (輸出符記)。\n",
        "\n"
      ],
      "metadata": {
        "id": "EUjg3lB_r3KQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 計算文字符記數量\n",
        "如果透過純文字輸入內容呼叫 count_tokens，則會傳回符記數量 則用於輸入內容 (total_tokens)。你可以先撥打這通電話，時間早於 呼叫 generate_content 檢查要求大小。\n",
        "\n",
        "另一種方式是呼叫 generate_content，然後使用 usage_metadata response 物件的屬性，就能取得下列資訊：\n",
        "\n",
        " - 輸入內容的個別符記數量 (prompt_token_count) 輸出內容 (candidates_token_count)\n",
        " - 輸入內容和輸出內容中的分詞總數 (total_token_count)"
      ],
      "metadata": {
        "id": "0gfN3aoLr_n6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\"models/gemini-1.5-flash\")\n",
        "\n",
        "prompt = \"敏捷的棕色狐狸跳過了那隻懶狗。\"\n",
        "\n",
        "# Call `count_tokens` to get the input token count (`total_tokens`).\n",
        "print(\"total_tokens: \", model.count_tokens(prompt))\n",
        "# ( total_tokens: 10 )\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "# On the response for `generate_content`, use `usage_metadata`\n",
        "# to get separate input and output token counts\n",
        "# (`prompt_token_count` and `candidates_token_count`, respectively),\n",
        "# as well as the combined token count (`total_token_count`).\n",
        "print(response.usage_metadata)\n",
        "# ( prompt_token_count: 11, candidates_token_count: 73, total_token_count: 84 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "MAPSGPRrmZkA",
        "outputId": "889735af-9690-4637-c36b-27dd61a4fdaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_tokens:  total_tokens: 12\n",
            "\n",
            "prompt_token_count: 13\n",
            "candidates_token_count: 116\n",
            "total_token_count: 129\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 計算多輪 (聊天) 符記數量\n",
        "如果透過即時通訊記錄呼叫 count_tokens，會傳回總權杖 即時通訊中每個角色的文字數量 (total_tokens)。\n",
        "\n",
        "另一種方式是呼叫 send_message，然後使用 usage_metadata response 物件的屬性，就能取得下列資訊：\n",
        "\n",
        " - 輸入內容的個別符記數量 (prompt_token_count) 輸出內容 (candidates_token_count)\n",
        " - 輸入內容和輸出內容中的分詞總數 (total_token_count)\n",
        "如要瞭解下次對話回合的長度，您需要在 並在呼叫 count_tokens 時納入記錄。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "amjsGqs6sKwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\"models/gemini-1.5-flash\")\n",
        "\n",
        "chat = model.start_chat(\n",
        "    history=[\n",
        "        {\"role\": \"user\", \"parts\": \"嗨，我的名字是凱文\"},\n",
        "        {\"role\": \"model\", \"parts\": \"嗨，凱文!\"},\n",
        "    ]\n",
        ")\n",
        "# Call `count_tokens` to get the input token count (`total_tokens`).\n",
        "print(model.count_tokens(chat.history))\n",
        "# ( total_tokens: 10 )\n",
        "\n",
        "response = chat.send_message(\n",
        "    \"用一句話向小朋友解釋計算機的工作原理。\"\n",
        ")\n",
        "\n",
        "# On the response for `send_message`, use `usage_metadata`\n",
        "# to get separate input and output token counts\n",
        "# (`prompt_token_count` and `candidates_token_count`, respectively),\n",
        "# as well as the combined token count (`total_token_count`).\n",
        "print(response.usage_metadata)\n",
        "# ( prompt_token_count: 25, candidates_token_count: 21, total_token_count: 46 )\n",
        "\n",
        "from google.generativeai.types.content_types import to_contents\n",
        "\n",
        "# You can call `count_tokens` on the combined history and content of the next turn.\n",
        "print(model.count_tokens(chat.history + to_contents(\"生命的意義是什麼？\")))\n",
        "# ( total_tokens: 56 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "DlLxz3YDmmkz",
        "outputId": "38d0ee19-8b45-43a9-94c9-531b7141619a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_tokens: 14\n",
            "\n",
            "prompt_token_count: 26\n",
            "candidates_token_count: 34\n",
            "total_token_count: 60\n",
            "\n",
            "total_tokens: 67\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 計算多模態符記\n",
        "提供給 Gemini API 的所有輸入內容都會權杖化，包括文字、圖片檔等 非文字形式的組合請注意以下關於權杖化的概略要點 的多模態輸入：\n",
        "\n",
        "系統會將圖片視為固定大小，因此可耗用一定數量的圖片 符記 (目前 258 個符記)，不論其顯示或檔案大小。\n",
        "\n",
        "影片和音訊檔案會按照下列固定費率轉換為權杖： 每秒有 263 個符記，音訊則每秒 32 個符記"
      ],
      "metadata": {
        "id": "CTVOiXnZsSZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "圖片檔\n",
        "在處理過程中，Gemini API 會將圖片視為固定大小， 使用固定數量的符記 (目前有 258 個符記) 螢幕或檔案大小\n",
        "\n",
        "如果使用文字和圖片輸入內容呼叫 count_tokens，系統會傳回合併後的 文字和圖像的符記數量僅限輸入內容 (total_tokens)。個人中心 可以先進行此呼叫，再呼叫 generate_content 檢查 要求。您也可以選擇對文字和檔案呼叫 count_tokens 。\n",
        "\n",
        "另一種方式是呼叫 generate_content，然後使用 usage_metadata response 物件的屬性，就能取得下列資訊：\n",
        "\n",
        " - 輸入內容的個別符記數量 (prompt_token_count) 輸出內容 (candidates_token_count)\n",
        " - 輸入內容和輸出內容中的分詞總數 (total_token_count)\n"
      ],
      "metadata": {
        "id": "FlfcybsssUCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\"models/gemini-1.5-flash\")\n",
        "\n",
        "prompt = \"告訴我關於這張圖片的內容\"\n",
        "your_image_file = genai.upload_file(path=\"/content/moon_text.png\")\n",
        "\n",
        "# Call `count_tokens` to get the input token count\n",
        "# of the combined text and file (`total_tokens`).\n",
        "# An image's display or file size does not affect its token count.\n",
        "# Optionally, you can call `count_tokens` for the text and file separately.\n",
        "print(model.count_tokens([prompt, your_image_file]))\n",
        "# ( total_tokens: 263 )\n",
        "\n",
        "response = model.generate_content([prompt, your_image_file])\n",
        "response.text\n",
        "# On the response for `generate_content`, use `usage_metadata`\n",
        "# to get separate input and output token counts\n",
        "# (`prompt_token_count` and `candidates_token_count`, respectively),\n",
        "# as well as the combined token count (`total_token_count`).\n",
        "print(response.usage_metadata)\n",
        "# ( prompt_token_count: 264, candidates_token_count: 80, total_token_count: 345 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "wozn1xU5nJQs",
        "outputId": "e32fd63e-4d27-4b34-fe6a-318a5996edb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_tokens: 266\n",
            "\n",
            "prompt_token_count: 267\n",
            "candidates_token_count: 79\n",
            "total_token_count: 346\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "\n",
        "model = genai.GenerativeModel(\"models/gemini-1.5-flash\")\n",
        "\n",
        "prompt = \"告訴我關於這張圖片的內容\"\n",
        "your_image_file = PIL.Image.open(\"/content/moon_text.png\")\n",
        "\n",
        "# Call `count_tokens` to get the input token count\n",
        "# of the combined text and file (`total_tokens`).\n",
        "# An image's display or file size does not affect its token count.\n",
        "# Optionally, you can call `count_tokens` for the text and file separately.\n",
        "print(model.count_tokens([prompt, your_image_file]))\n",
        "# ( total_tokens: 263 )\n",
        "\n",
        "response = model.generate_content([prompt, your_image_file])\n",
        "\n",
        "# On the response for `generate_content`, use `usage_metadata`\n",
        "# to get separate input and output token counts\n",
        "# (`prompt_token_count` and `candidates_token_count`, respectively),\n",
        "# as well as the combined token count (`total_token_count`).\n",
        "print(response.usage_metadata)\n",
        "# ( prompt_token_count: 264, candidates_token_count: 80, total_token_count: 345 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "ox0_8-r3pulh",
        "outputId": "c3a16a83-d44c-46b5-e51a-2eb4b92a9488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_tokens: 266\n",
            "\n",
            "prompt_token_count: 267\n",
            "candidates_token_count: 43\n",
            "total_token_count: 310\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 影片或音訊檔案\n",
        "系統每個音訊和影片都會按照下列固定費率轉換為符記：\n",
        "\n",
        " - 影片：每秒 263 個符記\n",
        " - 音訊：每秒 32 個符記\n"
      ],
      "metadata": {
        "id": "nToJi7IBtA46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果使用文字和影片/音訊輸入內容呼叫 count_tokens，系統會傳回 僅限輸入內容中文字和影片/音訊檔案的綜合符記數量 (total_tokens).您可以先撥打這場通話，再撥號給 generate_content 檢查請求的大小您也可以選擇在以下位置呼叫 count_tokens： 可以分別遷移文字和檔案\n",
        "\n",
        "另一種方式是呼叫 generate_content，然後使用 usage_metadata response 物件的屬性，就能取得下列資訊：\n",
        "\n",
        " - 輸入內容的個別符記數量 (prompt_token_count) 輸出內容 (candidates_token_count)\n",
        " - 輸入內容和輸出內容中的分詞總數 (total_token_count)\n"
      ],
      "metadata": {
        "id": "VBoyVA1utJ_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://storage.googleapis.com/generativeai-downloads/data/Sherlock_Jr_FullMovie.mp4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPdrmZtCqFnx",
        "outputId": "b882fb4f-201e-43ce-a389-7d8b0380d356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  316M  100  316M    0     0  20.7M      0  0:00:15  0:00:15 --:--:-- 27.9M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "model = genai.GenerativeModel(\"models/gemini-1.5-flash\")\n",
        "\n",
        "prompt = \"告訴我關於這個影片的內容\"\n",
        "your_file = genai.upload_file(path=\"/content/Sherlock_Jr_FullMovie.mp4\")\n",
        "\n",
        "# Videos need to be processed before you can use them.\n",
        "while your_file.state.name == \"PROCESSING\":\n",
        "    print(\"processing video...\")\n",
        "    time.sleep(5)\n",
        "    your_file = genai.get_file(your_file.name)\n",
        "\n",
        "# Call `count_tokens` to get the input token count\n",
        "# of the combined text and video/audio file (`total_tokens`).\n",
        "# A video or audio file is converted to tokens at a fixed rate of tokens per second.\n",
        "# Optionally, you can call `count_tokens` for the text and file separately.\n",
        "print(model.count_tokens([prompt, your_file]))\n",
        "# ( total_tokens: 300 )\n",
        "\n",
        "response = model.generate_content([prompt, your_file])\n",
        "\n",
        "# On the response for `generate_content`, use `usage_metadata`\n",
        "# to get separate input and output token counts\n",
        "# (`prompt_token_count` and `candidates_token_count`, respectively),\n",
        "# as well as the combined token count (`total_token_count`).\n",
        "print(response.usage_metadata)\n",
        "# ( prompt_token_count: 301, candidates_token_count: 60, total_token_count: 361 )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "8pKf9Vm-p90R",
        "outputId": "a3c98f66-5613-4ef6-a076-486d641d3093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing video...\n",
            "processing video...\n",
            "processing video...\n",
            "processing video...\n",
            "processing video...\n",
            "processing video...\n",
            "processing video...\n",
            "processing video...\n",
            "total_tokens: 696168\n",
            "\n",
            "prompt_token_count: 696169\n",
            "candidates_token_count: 682\n",
            "total_token_count: 696851\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 系統操作說明和工具\n",
        "系統操作說明和工具也會計入 。\n",
        "\n",
        "如果您採用系統指令，total_tokens 計數會增加以反映 增加 system_instruction。"
      ],
      "metadata": {
        "id": "iDS_Em2vtS_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "\n",
        "prompt = \"敏捷的棕色狐狸跳過了那隻懶狗。\"\n",
        "\n",
        "print(model.count_tokens(prompt))\n",
        "# total_tokens: 10\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\", system_instruction=\"你是一隻貓。你的名字是小灰。\"\n",
        ")\n",
        "\n",
        "# The total token count includes everything sent to the `generate_content` request.\n",
        "# When you use system instructions, the total token count increases.\n",
        "print(model.count_tokens(prompt))\n",
        "# ( total_tokens: 21 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A6Uekl1_qff9",
        "outputId": "2135eaa7-e593-44fe-cb68-3ad81692ef72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_tokens: 12\n",
            "\n",
            "total_tokens: 22\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果使用函式呼叫，total_tokens 計數會增加以反映 已新增 tools。"
      ],
      "metadata": {
        "id": "tr2SxXcgtYSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "\n",
        "prompt = \"我有 57 隻貓，每隻貓有 44 隻手套，總共有多少手套？\"\n",
        "\n",
        "print(model.count_tokens(prompt))\n",
        "# ( total_tokens: 22 )\n",
        "\n",
        "def add(a: float, b: float):\n",
        "    \"\"\"returns a + b.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "def subtract(a: float, b: float):\n",
        "    \"\"\"returns a - b.\"\"\"\n",
        "    return a - b\n",
        "\n",
        "def multiply(a: float, b: float):\n",
        "    \"\"\"returns a * b.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "def divide(a: float, b: float):\n",
        "    \"\"\"returns a / b.\"\"\"\n",
        "    return a / b\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    \"models/gemini-1.5-flash-001\", tools=[add, subtract, multiply, divide]\n",
        ")\n",
        "\n",
        "# The total token count includes everything sent to the `generate_content` request.\n",
        "# When you use tools (like function calling), the total token count increases.\n",
        "print(model.count_tokens(prompt))\n",
        "# ( total_tokens: 206 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Ps-ZWESeqzjR",
        "outputId": "9f4dbfed-a5fd-497c-94d9-9f2e23f9cf1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_tokens: 26\n",
            "\n",
            "total_tokens: 210\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-E0x5_GFAaPH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}